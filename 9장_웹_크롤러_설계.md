# 9장: 웹 크롤러 설계

* 웹 크롤러를 설계할 때, 고려해야 할 컴포넌트들은 아래와 같음
    * 예의 -> 하나의 호스트에 너무 자주 요청 보내지 않기(호스트마다 큐를 두고 큐 선택기를 통해 제어)
    * 거미덫 서버와 가치 없는 컨텐츠(블랙리스트, 필터링)
    * 중복 url, 컨텐츠 처리(블룸 필터)
    * 대량 url 처리(병렬 처리, 무상태 서버, 짧은 타임아웃)
    * 우선순위대로 크롤링(트래픽양 등을 기반으로 우선순위 큐에 배치, 우선순위 높은 큐를 더 자주 선택)
    * 크롤링 컨텐츠 타입 모두 지원(페이지에 포함된 데이터 타입 모두 얻어올 수 있게 작업 서버 인터페이스 마련)

* 추가로 고려하면 좋을 컴포넌트들은 아래와 같음
    * 서버사이드 렌더링: js를 이용해서 html 페이지 채우는 경우에는 그대로 파싱하면 빈 html 상태. 서버에서 렌더링해서 내용 채우고 파싱
    * 얻은 콘텐츠를 저장할 때, 샤딩으로 병렬성 확보&다중화로 유실 방지

## 웹 크롤러 흐름
1. 미수집 url 리스트에서 url을 가져옴
2. 우선순위 결정 장치에서 우선순위에 따라 큐에 배치
3. 큐 선택기에서 큐에 있는 url을 꺼냄. 이때, 우선순위가 높은 큐에서 더 자주 url 꺼내옴
4. 큐 라우터가 해당 url의 호스트에 맞는 큐로 url 배치시킴
5. 큐 선택기에서 예의 있는 웹 크롤러가 되도록 큐를 번갈아 가며 url 선택
6. 선택한 url을 html 조회 서버에 요청(DNS 캐싱, 짧은 타임아웃 설정으로 지연 단축)
7. html 파싱/검증으로 컴퓨터가 읽을 수 있고 내용 상 문제 없는거 확인
8. 중복 페이지가 아니면 콘텐츠 저장소에 저장
9. 해당 페이지에 있는 url 수집, 이미지 등의 콘텐츠 조회, 모니터링(저작권 침해 방지)
10. 수집된 url 중복 확인(블룸 필터) 
11. 아직 탐색되지 않은 url은 미수집 url 리스트로 이동